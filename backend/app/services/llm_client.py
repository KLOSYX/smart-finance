import datetime
import warnings
import asyncio

import pandas as pd

from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_experimental.agents import create_pandas_dataframe_agent

CATEGORIES = [
    "ä½æˆ¿",
    "é¤é¥®",
    "äº¤é€š",
    "å…¬ç”¨äº‹ä¸š",
    "è´­ç‰©",
    "å¨±ä¹",
    "å¥åº·ä¸å¥èº«",
    "æ—…è¡Œ",
    "æ•™è‚²",
    "å€ºåŠ¡",
    "å‚¨è“„/æŠ•èµ„",
    "éœ€è¦å¤æ ¸",
    "å…¶ä»–",
]

CATEGORIES_EN = [
    "Housing",
    "Food & Dining",
    "Transportation",
    "Utilities",
    "Shopping",
    "Entertainment",
    "Health & Fitness",
    "Travel",
    "Education",
    "Debt",
    "Savings/Investments",
    "Needs Review",
    "Other",
]


class OpenRouterChatOpenAI(ChatOpenAI):
    """
    Custom ChatOpenAI that preserves 'reasoning_details' in messages
    to satisfy OpenRouter/Gemini requirements.
    """

    def _convert_message_to_dict(self, message: BaseMessage) -> dict:
        message_dict = super()._convert_message_to_dict(message)

        # Check if we have additional_kwargs with reasoning_details
        if isinstance(message, AIMessage):
            reasoning_details = message.additional_kwargs.get("reasoning_details")
            if reasoning_details:
                message_dict["reasoning_details"] = reasoning_details

        return message_dict


def _get_llm(api_key, base_url, model, temperature=0.1):
    """
    Helper to create an OpenRouterChatOpenAI instance.
    """
    return OpenRouterChatOpenAI(
        api_key=api_key, base_url=base_url, model=model, temperature=temperature
    )


def _format_financial_context(monthly_income=0, investments=0):
    """
    Constructs a consistent context string for financial data.
    """
    context = ""
    if monthly_income > 0:
        context += f"User Monthly Income: Â¥{monthly_income:,.2f}. "
    if investments > 0:
        context += f"User Current Investments: Â¥{investments:,.2f}. "
    return context


def _get_agent_base_prompt(df_summary, language="zh"):
    """
    Returns the core instructions for the data analysis agent.
    """
    lang_instruction = (
        "Please answer in English." if language == "en" else "è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
    )

    return f"""
ä½ æ˜¯ä¸€ä½é«˜çº§è´¢åŠ¡æ•°æ®åˆ†æå¸ˆã€‚{lang_instruction}
æ‰€æœ‰è´§å¸å•ä½å‡ä¸ºäººæ°‘å¸ (Â¥)ã€‚æ­£æ•°è¡¨ç¤ºæ”¯å‡ºï¼Œè´Ÿæ•°è¡¨ç¤ºé€€æ¬¾ã€‚

ç¼–å†™/ä¿®æ”¹ pandas ä»£ç æ—¶è¯·ä½¿ç”¨ .loc é¿å…é“¾å¼èµ‹å€¼è­¦å‘Šï¼›å¦‚éœ€å¯¹åˆ‡ç‰‡ä¿®æ”¹ï¼Œè¯·å…ˆ copy()ã€‚
åœ¨ç¼–å†™ä»»ä½• pandas ä»£ç å‰è¯·ç¡®ä¿å…ˆæ‰§è¡Œ `import pandas as pd`ã€‚

æ•°æ®è¡¨æ¦‚è§ˆï¼ˆä¾›å‚è€ƒï¼Œè¯·å‹¿é‡å¤æ‰“å°å…¨è¡¨ï¼‰ï¼š{df_summary}
"""


def get_categories(language="zh"):
    return CATEGORIES_EN if language == "en" else CATEGORIES


def _chunk_text(text, max_chars=8000):
    if not text:
        return []
    lines = text.split("\n")
    chunks = []
    current_chunk = []
    current_length = 0
    for line in lines:
        line_len = len(line)
        if current_length + line_len > max_chars and current_chunk:
            chunks.append("\n".join(current_chunk))
            current_chunk = [line]
            current_length = line_len
        else:
            current_chunk.append(line)
            current_length += line_len + 1
    if current_chunk:
        chunks.append("\n".join(current_chunk))
    return chunks


async def _process_chunk_async(
    chunk, api_key, base_url, model_name, semaphore, language="zh"
):
    """
    Process a single chunk using LangChain asynchronously.
    """
    async with semaphore:
        try:
            llm = _get_llm(api_key, base_url, model_name, temperature=0.1)

            current_year = datetime.datetime.now().year

            target_categories = CATEGORIES_EN if language == "en" else CATEGORIES

            # Example category for the prompt (Shopping / è´­ç‰©)
            ex_category = target_categories[4]

            system_prompt = f"""
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è´¢åŠ¡åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æä¾›çš„æ–‡æœ¬ä¸­æå–ä¿¡ç”¨å¡äº¤æ˜“è¯¦æƒ…ï¼Œå¹¶å°†æ¯ç¬”äº¤æ˜“åˆ†ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š{", ".join(target_categories)}ã€‚

            ä¸¥æ ¼ä»¥JSONå¯¹è±¡åˆ—è¡¨çš„å½¢å¼è¿”å›è¾“å‡ºã€‚æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹é”®ï¼š
            - "Date": äº¤æ˜“æ—¥æœŸ (æ ¼å¼ YYYY-MM-DD)ã€‚å¦‚æœå¹´ä»½ç¼ºå¤±ï¼Œå‡è®¾ä¸º {current_year}ã€‚
            - "Description": å•†æˆ·åç§°æˆ–äº¤æ˜“æè¿°ã€‚
            - "Amount": äº¤æ˜“çš„æ•°å€¼ (æ­£æ•°è¡¨ç¤ºæ”¯å‡ºï¼Œè´Ÿæ•°è¡¨ç¤ºé€€æ¬¾ï¼Œå¿½ç•¥ä¿¡ç”¨å¡è¿˜æ¬¾)ã€‚
            - "Category": ä»æä¾›çš„ç±»åˆ«ä¸­é€‰æ‹©ä¸€ä¸ªã€‚
              - å¦‚æœæè¿°æ¨¡ç³Šä¸æ¸…æˆ–ä½ ä¸ç¡®å®šç±»åˆ«ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ "éœ€è¦å¤æ ¸"ã€‚
              - åªæœ‰å½“ä½ ç¡®å®šå®ƒä¸å±äºä¸Šè¿°ä»»ä½•ä¸»è¦ç±»åˆ«æ—¶ï¼Œæ‰ä½¿ç”¨ "å…¶ä»–"ã€‚
            - "CardLastFour": äº¤æ˜“å¡å·åå››ä½ã€‚å¦‚æœæœªæ‰¾åˆ°ï¼Œè¿”å› nullã€‚ä¾‹å¦‚ï¼š"8888"ã€‚

            åªè¿”å›JSONæ•°æ®ï¼Œä¸è¦æœ‰ä»»ä½•Markdownæ ¼å¼æˆ–è§£é‡Šã€‚
            ä¾‹å¦‚ï¼š[
                {{
                    "Date": "2023-01-01",
                    "Description": "è¶…å¸‚",
                    "Amount": 50.00,
                    "Category": "{ex_category}",
                    "CardLastFour": "1234"
                }}
            ]
            å¦‚æœæœªæ‰¾åˆ°äº¤æ˜“ï¼Œè¿”å› []ã€‚
            """

            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", "{system_prompt}"),
                    ("user", "Here is the statement text:\n\n{text}"),
                ]
            )

            # Chain
            chain = prompt | llm | JsonOutputParser()

            # Execute
            result = await chain.ainvoke(
                {"text": chunk, "system_prompt": system_prompt}
            )
            return result

        except Exception as e:
            print(f"Error processing chunk with LangChain: {e}")
            return []


async def analyze_transactions(text, api_key, base_url, model, language="zh"):
    """
    Sends the anonymized text to the LLM to extract and classify transactions using LangChain asynchronously.
    """
    print(f"DEBUG: Starting LangChain analysis with model='{model}'")
    chunks = _chunk_text(text)
    print(f"DEBUG: Text split into {len(chunks)} chunks. Processing in parallel...")

    all_transactions = []

    # Limit concurrent requests
    semaphore = asyncio.Semaphore(5)

    tasks = [
        _process_chunk_async(chunk, api_key, base_url, model, semaphore, language)
        for chunk in chunks
    ]

    results = await asyncio.gather(*tasks)

    for data in results:
        if data and isinstance(data, list):
            all_transactions.extend(data)

    print(f"DEBUG: Total transactions found: {len(all_transactions)}")
    return all_transactions


def _summarize_dataframe(df, max_cols=15):
    """
    Provide a lightweight description of the DataFrame to give the LLM context.
    """
    if df is None:
        return "æ— å¯ç”¨æ•°æ®è¡¨ã€‚"
    n_rows, n_cols = df.shape
    cols = list(df.columns)
    col_summaries = []
    for col in cols[:max_cols]:
        series = df[col]
        dtype = series.dtype
        nulls = int(series.isna().sum())
        col_summaries.append(f"{col} (dtype={dtype}, nulls={nulls})")
    if len(cols) > max_cols:
        col_summaries.append(f"...å¦å¤– {len(cols) - max_cols} åˆ—")
    columns_desc = "; ".join(col_summaries) if col_summaries else "æ— åˆ—ä¿¡æ¯"
    return f"è¡Œæ•°: {n_rows}ï¼Œåˆ—æ•°: {n_cols}ã€‚åˆ—ä¿¡æ¯: {columns_desc}"


def _extract_final_answer(text):
    """
    Trim agent chatter and keep only the final answer section.
    """
    if not isinstance(text, str):
        return text
    marker = "FINAL ANSWER:"
    idx = text.rfind(marker)
    if idx == -1:
        return text.strip()
    return text[idx:].strip()


def run_autonomous_agent(df, query, api_key, base_url, model, max_turns=20):
    """
    Pure executor for LangChain's Pandas DataFrame Agent.
    """
    print("DEBUG: Executing LangChain Pandas Agent...")

    try:
        # Work on a deep copy to reduce chained-assignment pitfalls from upstream slices.
        df_for_agent = df.copy(deep=True)
        llm = _get_llm(api_key, base_url, model, temperature=0)

        # Create the agent
        with warnings.catch_warnings():
            # Silence SettingWithCopyWarning that can surface from LLM-generated code.
            warnings.simplefilter("ignore", pd.errors.SettingWithCopyWarning)

            agent = create_pandas_dataframe_agent(
                llm,
                df_for_agent,
                verbose=True,
                agent_type="openai-tools",
                allow_dangerous_code=True,
                max_iterations=max_turns,
                handle_parsing_errors=True,
            )

        print(f"DEBUG: Invoking Agent with query: {query[:50]}...")
        response = agent.invoke(query)

        output = response.get("output", "Agent failed to generate output.")
        return _extract_final_answer(output)

    except Exception as e:
        print(f"Error running LangChain Agent: {e}")
        return f"Agent Error: {e}. Please try a different model."


def agentic_financial_advice(
    df, api_key, base_url, model, monthly_income=0, investments=0
):
    """
    Wrapper for the autonomous agent to generate financial advice.
    """
    df_summary = _summarize_dataframe(df)
    context_info = _format_financial_context(monthly_income, investments)
    base_prompt = _get_agent_base_prompt(df_summary)

    full_prompt = f"""
{base_prompt}

ä»»åŠ¡ç›®æ ‡ï¼šåŸºäºæä¾›çš„è´¢åŠ¡æ•°æ®æä¾›ä¸€ä»½å…¨é¢çš„è´¢åŠ¡å¥åº·æŠ¥å‘Šå’Œå¯æ“ä½œçš„å»ºè®®ã€‚
è´¢åŠ¡èƒŒæ™¯ï¼š{context_info}

åˆ†ææ­¥éª¤è¦æ±‚ï¼š
1. è¯†åˆ«æ”¯å‡ºæ’åå‰å‡ ä½çš„ç±»åˆ«å’Œå•†æˆ·ã€‚
2. åˆ†ææ”¯å‡ºè¶‹åŠ¿ï¼ˆä¾‹å¦‚ï¼šå‘¨æœ«ä¸å·¥ä½œæ—¥ã€é‡å¤æ€§è´¦å•ï¼‰ã€‚
3. è®¡ç®—å‚¨è“„ç‡ï¼ˆå¦‚æœæä¾›äº†æ”¶å…¥ä¿¡æ¯ï¼‰å¹¶è¯„ä¼°è´¢åŠ¡å¥åº·çŠ¶å†µã€‚
4. æ£€æµ‹ä»»ä½•å¼‚å¸¸æ”¯å‡ºã€‚

è¯·ç»™å‡º 3-5 æ¡å…·ä½“å»ºè®®ã€‚
"""

    return run_autonomous_agent(df, full_prompt, api_key, base_url, model)


def generate_financial_advice(
    summary_df, api_key, base_url, model, monthly_income=0, investments=0
):
    """
    Backward compatibility wrapper.
    """
    return agentic_financial_advice(
        summary_df, api_key, base_url, model, monthly_income, investments
    )


async def stream_autonomous_agent(df, query, api_key, base_url, model, max_turns=20):
    """
    Streaming executor for LangChain's Pandas DataFrame Agent.
    Yields chunks of the final answer.
    """
    print("DEBUG: Executing LangChain Pandas Agent (Streaming)...")

    try:
        # Work on a deep copy
        df_for_agent = df.copy(deep=True)
        llm = _get_llm(api_key, base_url, model, temperature=0)

        # Create the agent
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", pd.errors.SettingWithCopyWarning)
            agent = create_pandas_dataframe_agent(
                llm,
                df_for_agent,
                verbose=True,
                agent_type="openai-tools",
                allow_dangerous_code=True,
                max_iterations=max_turns,
                handle_parsing_errors=True,
            )

        print(f"DEBUG: Streaming Agent with query: {query[:50]}...")

        # Use astream_events to get granular updates
        # We perform a simple filter: only yield chunks from the final 'on_chat_model_stream'
        # or similar events that represent the final text output.
        # Alternatively, using agent.astream() with stream_mode="values" is often simpler for final output.
        # But 'openai-tools' agents are complex. Let's try agent.astream() which usually yields Input/Output dicts.
        # For actual token streaming, we need to iterate over the 'messages' from appropriate events.

        async for chunk in agent.astream_events({"input": query}, version="v2"):
            event = chunk["event"]

            # Debug: Log all events to understand structure (å¯ä»¥ä¹‹ååˆ é™¤)
            if "tool" in event.lower():
                print(
                    f"DEBUG TOOL EVENT: {event} | name={chunk.get('name')} | data keys={list(chunk.get('data', {}).keys())}"
                )

            # Tool Start
            if event == "on_tool_start":
                tool_name = chunk.get("name", "unknown")
                print(f"DEBUG: Yielding tool start for: {tool_name}")
                yield f"\n> ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}\n"

            # Tool End
            elif event == "on_tool_end":
                tool_name = chunk.get("name", "unknown")
                print(f"DEBUG: Yielding tool end for: {tool_name}")
                yield f"\n> âœ… å·¥å…· {tool_name} æ‰§è¡Œå®Œæ¯•\n"

            # 'on_chat_model_stream' gives us tokens from the LLM
            elif event == "on_chat_model_stream":
                data = chunk["data"]
                if "chunk" in data:
                    content = data["chunk"].content
                    if content:
                        yield content

    except Exception as e:
        print(f"Error running LangChain Agent (Stream): {e}")
        yield f"Agent Error: {e}. Please try a different model."


async def stream_chat_with_data(
    history,
    current_query,
    df,
    api_key,
    base_url,
    model,
    monthly_income=0,
    investments=0,
    language="zh",
):
    """
    Handles chat interaction using the LangChain Agent with Streaming.
    """
    df_summary = _summarize_dataframe(df)
    base_prompt = _get_agent_base_prompt(df_summary, language)
    fin_context = _format_financial_context(monthly_income, investments)

    history_str = ""
    if history:
        history_str = "Dialog History:\n" if language == "en" else "å¯¹è¯å†å²ï¼š\n"
        for msg in history[-5:]:
            role = (
                ("User" if msg["role"] == "user" else "AI")
                if language == "en"
                else ("ç”¨æˆ·" if msg["role"] == "user" else "AI")
            )
            history_str += f"{role}: {msg['content']}\n"

    # Keep prompt prompt structure in Chinese, just adapt the language requirement
    full_prompt = f"""
{base_prompt}

{history_str}
è´¢åŠ¡èƒŒæ™¯ï¼š{fin_context}

å½“å‰ç”¨æˆ·é—®é¢˜ï¼š"{current_query}"

ä»»åŠ¡è¦æ±‚ï¼š
1. å¦‚æœé—®é¢˜æ˜¯é—²èŠï¼Œè¯·ç¤¼è²Œåœ°å›ç­”ã€‚
2. å¦‚æœéœ€è¦æ•°æ®ï¼Œè¯·åˆ†æ DataFrame `df` å¹¶ç»“åˆè´¢åŠ¡èƒŒæ™¯ç»™å‡ºåˆ†æç»“è®ºã€‚
3. å¦‚æœéœ€è¦è¿›è¡Œè®¡ç®—ï¼Œè¯·è°ƒç”¨ pandas å·¥å…·ã€‚
"""
    # Delegate to the streaming executor
    async for token in stream_autonomous_agent(
        df, full_prompt, api_key, base_url, model
    ):
        yield token
